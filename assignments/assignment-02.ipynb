{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f457df4-a840-4838-9b30-fc5feb2c09b5",
   "metadata": {},
   "source": [
    "## ANLY-5800 Assignment 02\n",
    "\n",
    "#### ANLY-5800 (Fall '25)\n",
    "\n",
    "This assignment covers material from the recordings, notes, demos, and suggested readings from Lectures 3 - 6. \n",
    "\n",
    "**Format**: To submit this assignment, enter your answers directly in this document and submit it through Canvas.\n",
    "\n",
    "**Grade**: 10% (100 pts)\n",
    "\n",
    "---\n",
    "\n",
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369615c-54d7-4a72-bfe2-6fd84cef9582",
   "metadata": {},
   "source": [
    "### 1. Dropout (35 pts)\n",
    "\n",
    "Dropout is a regularization technique that randomly sets units in each activation layer, $a \\in \\mathbb{R}^{D}$, to zero and then multiplies the resultant vector elementwise by a constant $\\gamma$ according to:\n",
    "\n",
    "$$a' \\leftarrow  \\gamma m \\odot a$$\n",
    "\n",
    "where $\\odot$ represents the element-wise product operator and $m \\in \\{0, 1\\}^D$ is a mask with entries drawn from a Bernoulli distribution: \n",
    "\n",
    "$$m_i \\sim \\begin{cases} 0 &with \\quad P=p_{do} \\\\ 1 &with \\quad P=1 - p_{do} \\end{cases}$$\n",
    "\n",
    "Because dropout get's performed at training time and not during inference, blindly applying dropout will lead to a data distribution shift at inference. This is evident in how the hidden layer get's computed:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "z_{i}^{(l+1)} &= W_{i}^{(l+1)} \\cdot a^{'(l)} \\quad (training) \\\\\n",
    "z_{i}^{(l+1)} &= W_{i}^{(l+1)} \\cdot a^{(l)} \\quad (inference)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $W_{i}^{(l+1)} \\in \\mathbb{R}^{D}$. Because $z_{i}^{(l+1)}$ is computed as a weighted sum over the activations from the previous layer, given an identical input, the node values will shift in the absense of dropout unless we scale the activation values (almost certainly; the pathological exception is the case in which the values in $a^{(l)}$ come from a zero mean, zero skew distribution, which is unlikely). To ensure this doesn't happen, derive a training-time scaling constant, ${\\gamma}$, that will make each value $z_{i}^{(l+1)}$ invariant (in expectation) to the dropout operation.\n",
    "\n",
    "*Hint: You want to find the $\\gamma$ that makes the following true (in expectation). Some search terms that might come in handy: expectated value of a Bernoulli RV, the weak law of large numbers*.\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^{D-1} W_{i,j}^{(l+1)} a_{j}^{(l)}  = \\gamma \\sum_{j=0}^{D-1} W_{i,j}^{(l+1)} a_{j}^{'(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255826a",
   "metadata": {},
   "source": [
    "Your answer goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2785529-1d06-4474-8aec-baef38545787",
   "metadata": {},
   "source": [
    "### 2. Convolutions (30 pts)\n",
    "\n",
    "Consider a sequence of $T$ token embeddings, $Z \\in \\mathbb{R}^{T \\times D}$, for which $D=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347fcde5-c66a-4b4d-a7de-8491b5fcc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z = np.array([\n",
    "    [1.3,   0.4, -0.2],\n",
    "    [-3.1,  1.1,  2.1],\n",
    "    [0.9,   2.8, -1.5],\n",
    "    [1.3,   2.4,  0.1],\n",
    "    [1.0,   1.0,  0.5],\n",
    "    [3.0,  -1.4, -0.2],\n",
    "    [-0.7,  1.8,  1.3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3cbe3-5b3a-44aa-886f-9f56e126e04d",
   "metadata": {},
   "source": [
    "and a set of convolutional filters, $W=\\{ w^{(1)}, w^{(2)} \\}$, and corresponding filter widths $S=\\{ s^{(1)}, s^{(2)}  \\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437d0a4d-57ea-42c9-97d1-60055b3fbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "w2 = np.array([\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2]\n",
    "])\n",
    "\n",
    "W = [w1, w2]\n",
    "\n",
    "S = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c6909-6c25-4dde-8a87-fa79b27dbf3e",
   "metadata": {},
   "source": [
    "In Lecture 08 we discussed a set of operations that maps $Z \\in \\mathbb{R}^{T \\times D}$ onto $Z' \\in \\mathbb{R}^{N_F D}$ (in this problem $N_F = 2$). This involved three steps:\n",
    "\n",
    "1. **Convolution**: The convolutional operation produces $N_F$ feature maps, $B^{(n)} \\in \\mathbb{R}^{(T - s^{(n)} + 1) \\times D}$, where $n=\\{1, \\dots, N_F\\}$, according to:\n",
    "\n",
    "$$\n",
    "\\forall_{t \\in \\{ 1, \\dots, T - s^{(n)} + 1 \\} } \\; B^{(n)}_{t,j} = \\sum_{t'=1}^{S^{(n)}} w^{(n)}_{t',j} \\; Z_{t+t'-1, \\ j}\n",
    "$$\n",
    "\n",
    "2. **Max pooling**: The max pooling operation computes the max over the sequence dimension in each feature map, $ B_{maxpool}^{(n)} \\in \\mathbb{R}^D$, according to:\n",
    "\n",
    "$$\n",
    "B_{maxpool, j}^{(n)} = \\underset{1 \\leq t' \\leq T - s^{(n)} + 1 }{\\max} B^{(n)}_{t', j}\n",
    "$$\n",
    "\n",
    "3. **Concatenation**: The resultant set of $N_F$ feature vectors are then concatenated into a single vector $Z'$ according to:\n",
    "\n",
    "$$\n",
    "Z' = \\big[ B_{maxpool}^{(1)}, \\dots, B_{maxpool}^{(n)}, \\dots,  B_{maxpool}^{(N_F)}  \\big] \\in \\mathbb{R}^{D \\cdot N_F}\n",
    "$$\n",
    "\n",
    "In the cell below, perform these three operations to produce $Z' \\in \\mathbb{R}^6$ and print it.\n",
    "\n",
    "*Hint: The max pooling operation computes the maximum over each column in $B^{(n)}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d66b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b2167",
   "metadata": {},
   "source": [
    "### 3. Attention (35 pts)\n",
    "\n",
    "In this problem, you will take a pretrained language model's query, key, and value weight matrices to compute a simple self-attention layer, and then produce a plot of the resultant attention weights, for a single input sequence. \n",
    "\n",
    "The input array $X$ is an $T \\times D_x$ matrix where $T$ is the number of tokens in your input sequence, and $D_x$ is the dimension of the token embedding. The query mapping $W_q$ is shape $D_x \\times D_q$, where $D_q$ is the query dimension. The key mapping $W_k$ is also shape $D_x \\times D_q$, and the value mapping $W_v$ is shape $D_x \\times D_v$, where $D_v$ is the value dimension. These mappings act on the input aray $X$ to produce the query, key, and value matrices (with associated biases):\n",
    "\n",
    "$$ Q = XW_q + b_q, \\quad K = XW_k + b_k, \\quad \\text{and} \\quad V = XW_v + b_v.$$\n",
    "\n",
    "With the above computations, we compute the alignments as $E = QK^T / sqrt(D_q)$, and from these unnormalized scores, we obtain attention weights $A$ passing through a softmax: $A = \\textrm{softmax}(E)$. Outputs are obtained by computing $Y= A V.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf848079",
   "metadata": {},
   "source": [
    "\n",
    "#### Part A \n",
    "For the sequence `The quick brown fox jumps over the lazy dog`, compute the self-attention weights using the operations described above for the DistilBERT model (https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France). You will have to tokenize the inputs in the same way the model was trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Your answer goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055187ae",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "For each token in the sequence from (A), visualize its attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through generator to grab weights and biases\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"transformer.layer.5.attention.q_lin.weight\":\n",
    "        W_q = param\n",
    "    if name == \"transformer.layer.5.attention.q_lin.bias\":\n",
    "        b_q = param\n",
    "    if name == \"transformer.layer.5.attention.k_lin.weight\":\n",
    "        W_k = param\n",
    "    if name == \"transformer.layer.5.attention.k_lin.bias\":\n",
    "        b_k = param\n",
    "    if name == \"transformer.layer.5.attention.v_lin.weight\":\n",
    "        W_v = param\n",
    "    if name == \"transformer.layer.5.attention.v_lin.bias\":\n",
    "        b_v = param\n",
    "\n",
    "# Your answer goes here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
