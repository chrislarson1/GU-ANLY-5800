<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 01</title>
    <meta charset="utf-8">
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <!-- <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          CommonHTML: {
            scale: 100
          }
        });
    </script> -->
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            commonHTML: {
                scale: 100
            },
            extensions: ["tex2jax.js"],
            jax: [
                "input/TeX",
                "output/HTML-CSS"
            ],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            "HTML-CSS": { fonts: ["STIX"] }
        });
    </script>
    <!-- <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->
    <link rel="stylesheet" type="text/css" href="https://chrislarson1.github.io/remark-formatting/style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle

# ANLY-5800
# Advanced Natural Language Processing

#### Lecture 01

Math Foundations

<br/>

Georgetown University

Fall 2025

---
## Lesson plan
- Course overview
- Course logistics
- Math refresher

---

## Instructor: Chris Larson

#### Education

B.A. | Coe College | Physics

Ph.D. | Cornell University | Theoretical & Applied Mechanics, Computer Science

#### Professional Experience

**AnyTeam** | Founding Team | LLM agents that run on your desktop | 2024 - present
<br> In stealth, seed round ($10M)

**Virtex Labs** | Founder | k8s control plane LLM agent |  2023 - present
<br>Self funded

**StormForge** | Lead Engineer | k8s optimization tools | 2020 - 2023
<br>Series A,B ($80M) | Acquired by CloudBolt

**Capital One** | Lead SWE, Sr. Manager (ML) | Conversational AI, Eno | 2017 - 2020

**Corning Inc.** | Sr. Research Engineer | Research & Development | 2011 - 2013

**NASA (United Space Alliance)** | Engineer II | Modeling & simulation | 2009-2011

---
class: center, middle

## [Course website](https://github.com/chrislarson1/GU-ANLY-5800)

---

## What this course offers you

- Math foundations → You will build a much deeper understanding of probability, statistics, and machine learning.

- Programming skills → You will sharpen your ability to express mathematical ideas in code.

- Domain knowledge → You will build a well rounded tool chest to solve problems involving text and sequential data. This captures a surprisingly large class of problems.

- Job prospects → The topics covered in this class constitute a core set of skills that are hard to come by!

---

## Eras of Natural Language Processing (NLP)

### Incubatory period
- 1950s–1990s: Rule-based systems — grammars, logic, symbolic AI
- 1990s–2010: Statistical modeling — n-grams, HMMs, SVMs, Bayes

### Systems that understand natural language
- 2010–2017: Neural language modeling — word vectors, RNNs, CNNs, seq2seq
- 2017–2022: Transformers — pretraining, transfer learning, attention

### Systems that reason and act using natural language
- 2023–present: LLMs & Agents — tools, reasoning, software 3.0

---

## Weeks 1–8

- Week 1: Mathematical Foundations — linear algebra, probability, information
- Week 2: Decision Boundary Learning — perceptron, SVMs, kernels
- Week 3: Parameter Estimation — MLE, MAP, taxonomy of stat ML approaches
- Week 4: Distributional Semantics — TF‑IDF, PMI, LSA, LDA, Word2Vec
- Week 5: Neural Networks — backprop, optimization, regularization, scheduling
- Week 6: Language Modeling — n‑grams, HMMs, autoregression, EBMs, RNNs
- Week 7: Sequence Models — seq2seq, attention, memory & pointer mechanisms
- Week 8: Transformers — self/multi head attention, tokenization, autoregr.

---

## Weeks 9–15

- Week 09: LLM Finetuning & Alignment — SFT, instruction tuning, RL, scaling
- Week 10: Modern Architectures — MoE, SSM, JEM, sparse nets, contrastive nets
- Week 11: Cross‑Domain — vision, speech, multimodal, world models
- Week 12: Reasoning — CoT, ToT, causal inference, program synthesis
- Week 13: Search & Retrieval — DPR, RAG, multi‑hop, graph retrieval
- Week 14: Agents — tools & function calling, MCP, ReAct framework
- Week 15: Training & Inference Compute — quantization, KV cache, types of parallelism

---

class: center, middle

#### [Math Refresher](https://github.com/chrislarson1/GU-ANLY-5800/blob/main/lectures/01/lecture-01.md)

    </textarea>
    <div class="footer"><p>Georgetown University | Fall 2025 | ANLY 5800 | Lecture 01</p></div>
    <script> var slideshow = remark.create(); </script>
</body>


</html>